from typing import Optional, Type, Dict, Any, Callable
from os.path import join, isfile, exists, sep
from datetime import datetime
from vedo import ProgressBar
from torch.nn import Module
from torch.optim import Optimizer

from DeepPhysX.database.database_manager import DatabaseManager
from DeepPhysX.networks.network_manager import NetworkManager
from DeepPhysX.networks.stats_manager import StatsManager
from DeepPhysX.simulation.simulation_manager import SimulationManager
from DeepPhysX.utils.path import create_dir, get_session_dir


class TrainingPipeline:

    def __init__(self,
                 network_manager: NetworkManager,
                 database_manager: DatabaseManager,
                 loss_fnc: Type[Module],
                 optimizer: Type[Optimizer],
                 loss_kwargs: Optional[Dict[str, Any]] = None,
                 optimizer_kwargs: Optional[Dict[str, Any]] = None,
                 simulation_manager: Optional[SimulationManager] = None,
                 new_session: bool = True,
                 session_dir: str = 'sessions',
                 session_name: str = 'training',
                 epoch_nb: int = 0,
                 batch_nb: int = 0,
                 batch_size: int = 0,
                 use_tensorboard: bool = True,
                 save_intermediate_state_every: int = 0):
        """
        TrainingPipeline implements the main loop that trains a neural network from simulation data.
        Data can be pre-computed or generated on the fly.

        :param network_manager: Manager for the Network.
        :param database_manager: Manager for the Database.
        :param loss_fnc: Loss function to use in the optimization process.
        :param optimizer: Optimizer used for training.
        :param loss_kwargs: Dict of kwargs to create an instance of the loss.
        :param optimizer_kwargs: Dict of kwargs to create an instance of the optimizer.
        :param simulation_manager: Manager for the numerical Simulation.
        :param new_session: If True, a new repository is created for the session.
        :param session_dir: Path to the directory that contains the DeepPhysX session repositories.
        :param session_name: Name of the current session repository.
        :param epoch_nb: Number of epochs to perform.
        :param batch_nb: Number of batches to use.
        :param batch_size: Number of samples to produce per batch.
        :param use_tensorboard: If True, display training curves in tensorboard.
        :param save_intermediate_state_every: Save the Network state periodically if > 1.
        """

        # Create a new session if required
        self.session_dir = get_session_dir(session_dir, new_session)
        if not new_session:
            new_session = not exists(join(self.session_dir, session_name))
        if new_session:
            session_name = create_dir(session_dir=self.session_dir,
                                      session_name=session_name).split(sep)[-1]
        self.produce_data = database_manager.existing_dir is None

        # Create a DatabaseManager
        self.database_manager = database_manager
        self.database_manager.init_training_pipeline(session=join(self.session_dir, session_name),
                                                     new_session=new_session,
                                                     produce_data=self.produce_data)

        # Create a SimulationManager
        self.simulation_manager = None
        if simulation_manager is not None:
            self.simulation_manager = simulation_manager
            self.simulation_manager.init_training_pipeline(batch_size=batch_size)
            self.simulation_manager.connect_to_database(database_path=(self.database_manager.database_dir, 'dataset'),
                                                        normalize_data=self.database_manager.normalize)

        # Create a NetworkManager
        self.network_manager = network_manager
        self.network_manager.init_training_pipeline(loss_fnc=loss_fnc,
                                                    loss_kwargs=loss_kwargs,
                                                    optimizer=optimizer,
                                                    optimizer_kwargs=optimizer_kwargs,
                                                    new_session=new_session,
                                                    session=join(self.session_dir, session_name),
                                                    save_intermediate_state_every=save_intermediate_state_every)
        self.network_manager.connect_to_database(database_path=(self.database_manager.database_dir, 'dataset'),
                                                 normalize_data=self.database_manager.normalize)
        if self.simulation_manager is not None:
            self.simulation_manager.connect_to_network_manager(network_manager=self.network_manager)

        # Create a StatsManager
        self.stats_manager = StatsManager(session=join(self.session_dir, session_name)) if use_tensorboard else None

        # Training variables
        self.epoch_nb = epoch_nb
        self.epoch_id = 0
        self.batch_nb = batch_nb
        self.batch_size = batch_size
        self.batch_id = 0
        self.nb_samples = batch_nb * batch_size * epoch_nb
        self.loss_dict = None

        # Progressbar
        self.digits = ['{' + f':0{len(str(self.epoch_nb))}d' + '}',
                       '{' + f':0{len(str(self.batch_nb))}d' + '}']
        epoch_id, epoch_nb = self.digits[0].format(0), self.digits[0].format(self.epoch_nb)
        batch_id, batch_nb = self.digits[1].format(0), self.digits[1].format(self.batch_nb)
        self.progress_bar = ProgressBar(start=0, stop=self.batch_nb * self.epoch_nb, c='orange',
                                        title=f'Epoch n째{epoch_id}/{epoch_nb} - Batch n째{batch_id}/{batch_nb}')

        # Save info file
        filename = join(join(self.session_dir, session_name), 'info.txt')
        date_time = datetime.now().strftime('%d/%m/%Y %H:%M:%S')
        if not isfile(filename):
            f = open(filename, "w+")
            # Session description template for user
            f.write("## DeepPhysX Training Session ##\n")
            f.write(date_time + "\n\n")
            f.write("Personal notes on the training session:\nnetworks Input:\nnetworks Output:\nComments:\n\n")
            # Listing every component descriptions
            f.write("## List of Components Parameters ##\n")
            f.write(str(self))
            f.write(str(self.network_manager))

            if self.simulation_manager is not None:
                f.write(str(self.simulation_manager))
            if self.database_manager is not None:
                f.write(str(self.database_manager))

            if self.stats_manager is not None:
                f.write(str(self.stats_manager))
            f.close()

    def execute(self, user_training_loop: Optional[Callable] = None) -> None:
        """
        Launch the training Pipeline.
        """

        self.__default_training_loop() if user_training_loop is None else user_training_loop()
        # Training end
        for manager in (self.database_manager, self.network_manager, self.stats_manager, self.simulation_manager):
            if manager is not None:
                manager.close()


    def __default_training_loop(self) -> None:
        """
        Default training loop if no training function was set by user.
        """

        # Epoch condition
        while self.epoch_id < self.epoch_nb:

            # Epoch begin
            self.batch_id = 0

            # Batch condition
            while self.batch_id < self.batch_nb:

                # Batch begin
                id_epoch, nb_epoch = self.digits[0].format(self.epoch_id + 1), self.digits[0].format(self.epoch_nb)
                id_batch, nb_batch = self.digits[1].format(self.batch_id + 1), self.digits[1].format(self.batch_nb)
                self.progress_bar.title = f'Epoch n째{id_epoch}/{nb_epoch} - Batch n째{id_batch}/{nb_batch} '
                self.progress_bar.print()

                # Get data from Environment(s) if used and if the data should be created at this epoch
                if self.simulation_manager is not None and self.produce_data and \
                        (self.epoch_id == 0 or self.simulation_manager.always_produce):

                    self.data_lines = self.simulation_manager.get_data(animate=True)
                    self.database_manager.add_data(self.data_lines)

                # Get data from Dataset
                else:
                    self.data_lines = self.database_manager.get_data(batch_size=self.batch_size)
                    # Dispatch a batch to clients
                    if self.simulation_manager is not None:
                        if self.simulation_manager.load_samples and \
                                (self.epoch_id == 0 or not self.simulation_manager.only_first_epoch):
                            self.simulation_manager.dispatch_batch(data_lines=self.data_lines,
                                                                   animate=True)
                        # Environment is no longer used
                        else:
                            self.simulation_manager.close()
                            self.simulation_manager = None

                # Optimize
                batch_fwd, batch_bwd = self.network_manager.get_data(lines_id=self.data_lines)
                net_predict = self.network_manager.get_predict(batch_fwd=batch_fwd)
                loss = self.network_manager.get_loss(net_predict=net_predict, batch_bwd=batch_bwd)
                self.network_manager.optimize()

                # Batch end
                self.batch_id += 1
                if self.stats_manager is not None:
                    self.stats_manager.add_train_batch_loss(loss,
                                                            self.epoch_id * self.batch_nb + self.batch_id)

            # Epoch end
            self.epoch_id += 1
            if self.simulation_manager is not None and self.produce_data and \
                    (self.epoch_id == 0 or self.simulation_manager.always_produce):
                self.database_manager.compute_normalization()
                self.network_manager.reload_normalization()
            if self.stats_manager is not None:
                self.stats_manager.add_train_epoch_loss(loss, self.epoch_id)
            self.network_manager.save_network()

    def __str__(self):

        description = "\n"
        description += f"# {self.__class__.__name__}\n"
        description += f"    Session repository: {self.session_dir}\n"
        description += f"    Number of epochs: {self.epoch_nb}\n"
        description += f"    Number of batches per epoch: {self.batch_nb}\n"
        description += f"    Number of samples per batch: {self.batch_size}\n"
        description += f"    Number of samples per epoch: {self.batch_nb * self.batch_size}\n"
        description += f"    Total: Number of batches : {self.batch_nb * self.epoch_nb}\n"
        description += f"           Number of samples : {self.nb_samples}\n"
        return description
